{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis on reviews data\n",
    "Kei Sato\n",
    "\n",
    "ML310B - Advanced Machine Learning\n",
    "\n",
    "March 25, 2019\n",
    "\n",
    "#### Project overview\n",
    "For this assignment, we want to use sentiment analysis to predict the polarity of a given film review.  To build the model, we are given a corpus of 50K reviews, each associated with a score of 0 or 1, which respectively indicate that the review is negative or positive.  \n",
    "\n",
    "#### Metrics used\n",
    "We will use accuracy as the main metric used to determine if the model is successful.  But, throughout the model training and cross validation, the proportion of false positives for both classes will be monitored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews \n",
      " 1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data...\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('resources/Reviews.csv')\n",
    "print(\"Number of positive and negative reviews\", '\\n', data[\"sentiment\"].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML tags\n",
    "Original sentence:\n",
    "```\n",
    "<br /><br />The trailer of \"\"Nasaan ka man\"\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon.\n",
    "```\n",
    "\n",
    "Removed HTML tags:\n",
    "```\n",
    "The trailer of \"\"Nasaan ka man\"\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# takes string, returns string\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all text to lowercase\n",
    "Original sentence:\n",
    "```\n",
    "The trailer of \"\"Nasaan ka man\"\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon.\n",
    "```\n",
    "Lowercase:\n",
    "```\n",
    "the trailer of nasaan ka man caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes string, returns string\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand contractions\n",
    "Original text:\n",
    "```\n",
    "The SF premise isn't unique (although it pretty much was back then)\n",
    "```\n",
    "\n",
    "Without contractions:\n",
    "```\n",
    "The SF premise is not unique (although it pretty much was back then)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tag\n",
    "import json\n",
    "\n",
    "with open('resources/contractions.json', 'r') as f:\n",
    "    contractions = json.load(f)\n",
    "contractions_keys = contractions.keys()\n",
    "\n",
    "# takes tokenized text, returns tokenized text\n",
    "def expand_contractions(text):\n",
    "    text = text.split()\n",
    "    return ' '.join(list(map(lambda word: contractions[word] if word in contractions_keys else word, text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove symbols and punctuation\n",
    "Original text:\n",
    "```\n",
    "though it makes the most sophisticated use of the \"\"cut-out\"\" method of animation (a la \"\"south sark\"\"), the real talent behind\n",
    "```\n",
    "Removed symbols and punctuation:\n",
    "```\n",
    "though it makes the most sophisticated use of the cutout method of animation a la south park the real talent behind\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replace_re_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "delete_re_symbols = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def remove_symbols_punctuation(text):\n",
    "    text = re.sub(delete_re_symbols.pattern, '', text)\n",
    "    text = re.sub(replace_re_by_space.pattern, ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "Original text:\n",
    "```\n",
    "I went to this movie tonight with a few friends not knowing more than the Actors that were in it, and that it was supposed to be a horror movie.\n",
    "```\n",
    "Removed stop words:\n",
    "```\n",
    "I went movie tonight friends knowing Actors , supposed horror movie\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    filtered_sentence = [w for w in text if not w in stop_words]\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Lemmatization\n",
    "Original text:\n",
    "```\n",
    "I went to this movie tonight with a few friends not knowing more than the Actors that were in it, and that it was supposed to be a horror movie.\n",
    "```\n",
    "Lemmatization applied:\n",
    "```\n",
    "I went movie tonight friends knowing Actors , supposed horror movie\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def text_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = list(map(lambda word: wordnet_lemmatizer.lemmatize(word), text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Text Processing\n",
    "The reviews corpus has 50,000 reviews and is evenly split between positive and negative reviews, so that it contains 25,000 positive and 25,000 negative reviews.  Before doing any more data exploration, we process the text using standard techniques.  Much of this code was taken from the Lesson 8 HW assignment.\n",
    "\n",
    "The first step is apply some basic text processing, it was done in the following order.\n",
    "1.  Remove proper nouns:  This was done by using the NLTK position tagging functionality to identify proper nouns.\n",
    "2.  Expand contractions\n",
    "3.  Convert all text to lowercase\n",
    "4.  Remove `<br />` characters, this was because the `<br />` HTML tag was present in many reviews.  This part of cleaning the text was specific to this corpus.\n",
    "5.  Remove symbols and punctuation\n",
    "6.  Remove stop words.  For this application, I also removed the words \"movie\" and \"film\" because they were occured very often throughout positive and negative reviews.\n",
    "\n",
    "After cleaning the text, lemmatization is applied.  I did try to apply stemming to the dataset, but that produced too many non words and so it has been omitted from the text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cleaning data\n"
     ]
    }
   ],
   "source": [
    "# Taken Lesson 8 HW assignment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "# these are only to be run once\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# converts to lowercase and removes <br />, punctuation, stop words, and numbers\n",
    "def text_processing(text):\n",
    "    # remove HTML\n",
    "    text = strip_html(text)\n",
    "    \n",
    "    # expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    # lower case letters\n",
    "    text = lowercase(text)\n",
    "    \n",
    "    # remove punctuation/symbols\n",
    "    text = remove_symbols_punctuation(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # text lemmatization\n",
    "    text = text_lemmatization(text)\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "test_data = data.copy(deep=True)\n",
    "test_data[\"review\"] = test_data[\"review\"].apply(lambda text: text_processing(text))\n",
    "print(\"done cleaning data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration\n",
    "Below is some initial data exploration.  We can see that the average length of positive and negative reviews is roughtly the same.  The ten most frequently occuring words are also very similar across between the sets of positive and negative reviews.  I also outputted the ten least commonly occuring words, in part for my own curiosity and to verify that the ten least commonly occuring words were still complete words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of negative reviews: 116\n",
      "Average word count of positive reviews: 119\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter \n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "\n",
    "# Get average length of reviews\n",
    "def get_avg_length_review(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    avg_review_length = list(map(lambda review: len(review.split()), relevant_reviews))\n",
    "    return int(np.mean(avg_review_length))\n",
    "print(\"Average word count of negative reviews:\", get_avg_length_review(test_data, 0))\n",
    "print(\"Average word count of positive reviews:\", get_avg_length_review(test_data, 1))\n",
    "\n",
    "# Get 10 most and least frequently occuring words, verify that real words are coming through\n",
    "def get_most_least_common_words(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    all_relevant_reviews = reduce(lambda accum, curr: accum + curr, relevant_reviews)\n",
    "    counted_words = Counter(all_relevant_reviews.split())\n",
    "    most_common = counted_words.most_common(10)\n",
    "    least_common = heapq.nsmallest(10, counted_words.items(), key=itemgetter(1))\n",
    "    return most_common, least_common\n",
    "\n",
    "negative_reviews = get_most_least_common_words(test_data, 0)\n",
    "positive_reviews = get_most_least_common_words(test_data, 1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"10 most common words in negative reviews:\", negative_reviews[0])\n",
    "print(\"10 least common words in negative reviews:\", negative_reviews[1])\n",
    "print('\\n')\n",
    "print(\"10 most common words in positive reviews:\", positive_reviews[0])\n",
    "print(\"10 least common words in positive reviews:\", positive_reviews[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology\n",
    "The data transformed by using TFIDIF then fed into a Logistic Regression classifier.  I am using a test train split of 30% and 70%. Most settings were using the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "def get_incorrect_predictions(data, y_true, y_pred):\n",
    "    predicted_pos = 0\n",
    "    predicted_neg = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = pd.DataFrame({'review': [], 'sentiment': []})\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct_predictions+=1\n",
    "        else:\n",
    "            incorrect_predictions.loc[len(incorrect_predictions)] = [data[i], y_pred[i]]\n",
    "            if y_pred[i] == 1:\n",
    "                predicted_pos+=1\n",
    "            else:\n",
    "                predicted_neg+=1\n",
    "    print(\"Predicted POSITIVE, actually NEGATIVE\", round(float(predicted_pos)/float(len(y_true)), 3))\n",
    "    print(\"Predicted NEGATIVE, actually POSITIVE\", round(float(predicted_neg)/float(len(y_true)), 3))\n",
    "    print('\\n')\n",
    "    return incorrect_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# test_data[\"review\"].apply(lambda text: text_processing(text))\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    test_data[\"review\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('done splitting data')\n",
    "\n",
    "# transform data\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.0005, ngram_range=(1,2)).fit(x_train)\n",
    "\n",
    "print('done fitting vectorizer')\n",
    "\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "print('done transforming text')\n",
    "\n",
    "# train model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('done fitting model, will start scoring')\n",
    "\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print('model score', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n-gram|min document freq|accuracy|\n",
    "|------|------|-----|\n",
    "|(1, 1)| 0.0001|0.884|\n",
    "|(1, 1)| 0.00025|0.884|\n",
    "|(1, 1)| 0.0005|0.885|\n",
    "|(1, 1)| 0.001|0.883|\n",
    "|(1, 1)| 0.0025|0.88|\n",
    "|(1, 2)| 0.0001|n/a|\n",
    "|(1, 2)| 0.00025|n/a|\n",
    "|(1, 2)| 0.0005|0.889|\n",
    "|(1, 2)| 0.001|0.887|\n",
    "|(1, 2)| 0.0025|0.883|\n",
    "|(2, 2)| 0.0001|n/a|\n",
    "|(2, 2)| 0.00025|n/a|\n",
    "|(2, 2)| 0.0005|0.816|\n",
    "|(2, 2)| 0.001|0.793|\n",
    "|(2, 2)| 0.0025|0.743|\n",
    "|(2, 3)| 0.0001|n/a|\n",
    "|(2, 3)| 0.00025|n/a|\n",
    "|(2, 3)| 0.0005|0.816|\n",
    "|(2, 3)| 0.001|0.792|\n",
    "|(2, 3)| 0.0025|0.743|\n",
    "|(2, 4)| 0.0001|n/a|\n",
    "|(2, 4)| 0.00025|n/a|\n",
    "|(2, 4)| 0.0005|0.816|\n",
    "|(2, 4)| 0.001|0.792|\n",
    "|(2, 4)| 0.0025|0.743|\n",
    "|(3, 3)| 0.0001|0.675|\n",
    "|(3, 3)| 0.00025|0.633|\n",
    "|(3, 3)| 0.0005|0.585|\n",
    "|(3, 3)| 0.001|0.554|\n",
    "|(3, 3)| 0.0025|0.552|\n",
    "|(3, 4)| 0.0001|0.675|\n",
    "|(3, 4)| 0.00025|0.633|\n",
    "|(3, 4)| 0.0005|0.585|\n",
    "|(3, 4)| 0.001|0.554|\n",
    "|(3, 4)| 0.0025|0.522|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further analysis\n",
    "While this model was able to achieve a roughly 90% accuracy across both classes, there are many more ways to improve on this, either by doing more feature engineering or using a more robust model.\n",
    "\n",
    "Different models:  It would be worthwhile to try different models on the dataset.  An SVM would be appropriate because this is a binary classification problem.  However, I am also interested in the effects of measuring document similarity and using that for a clustering model.\n",
    "\n",
    "Word attributes:  There are other qualities of the individual words that could be further processed.  For example, whether or not the average word length of a review is correlated to the sentiment of the review.  Other aspects could include the obscurity and if the words are mispelled.\n",
    "\n",
    "Stemming:  I chose not to include stemming because the nltk libraries were producing too many non words, but it would definitely be worthwhile to invest more time into applying stemming correctly.\n",
    "\n",
    "Sentence structure:  The sentence structure used could be indicative of the document's sentiment.  One hypothesis is that negative reviews have more sentences written in the first person, such beginning with \"I think ...\"  We can also explore if positive or negative reviews are correlated with incorrectly or correctly structured sentences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "NLTK book http://www.nltk.org/book/\n",
    "\n",
    "Blog post on sentiment analysis https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
